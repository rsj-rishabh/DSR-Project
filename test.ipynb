{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_reader = pd.read_json('yelp_academic_dataset_review.json', chunksize=1, lines=True)\n",
    "#business_reader = pd.read_json('yelp_academic_dataset_business.json', chunksize=1, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching business data (1000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#df = []\n",
    "#for chunk in business_reader:\n",
    "#    #print(chunk)\n",
    "#    df.append(pd.DataFrame(chunk))\n",
    "#    i = i+1\n",
    "#    if i==1000:\n",
    "#        break\n",
    "#business_data = pd.concat(df)\n",
    "#business_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching review data (100 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iCQpiavjjPzJ5_3gPD5Ebg</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-25</td>\n",
       "      <td>0</td>\n",
       "      <td>x7mDIiDB3jEiPGPHOmDzyw</td>\n",
       "      <td>2</td>\n",
       "      <td>The pizza was okay. Not the best I've had. I p...</td>\n",
       "      <td>0</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pomGBqfbxcqPv14c3XH-ZQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-11-13</td>\n",
       "      <td>0</td>\n",
       "      <td>dDl8zu1vWPdKGihJrwQbpw</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this place! My fiance And I go here atl...</td>\n",
       "      <td>0</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jtQARsP6P-LbkyjbO1qNGg</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>1</td>\n",
       "      <td>LZp4UX5zK3e-c5ZGSeo3kA</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible. Dry corn bread. Rib tips were all fa...</td>\n",
       "      <td>3</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elqbBhBfElMNSrjFqW3now</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-25</td>\n",
       "      <td>0</td>\n",
       "      <td>Er4NBWCmCD4nM8_p1GRdow</td>\n",
       "      <td>2</td>\n",
       "      <td>Back in 2005-2007 this place was my FAVORITE t...</td>\n",
       "      <td>2</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ums3gaP2qM3W1XcA5r6SsQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-09-05</td>\n",
       "      <td>0</td>\n",
       "      <td>jsDu6QEJHbwP2Blom1PLCA</td>\n",
       "      <td>5</td>\n",
       "      <td>Delicious healthy food. The steak is amazing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  iCQpiavjjPzJ5_3gPD5Ebg     0 2011-02-25      0  x7mDIiDB3jEiPGPHOmDzyw   \n",
       "1  pomGBqfbxcqPv14c3XH-ZQ     0 2012-11-13      0  dDl8zu1vWPdKGihJrwQbpw   \n",
       "2  jtQARsP6P-LbkyjbO1qNGg     1 2014-10-23      1  LZp4UX5zK3e-c5ZGSeo3kA   \n",
       "3  elqbBhBfElMNSrjFqW3now     0 2011-02-25      0  Er4NBWCmCD4nM8_p1GRdow   \n",
       "4  Ums3gaP2qM3W1XcA5r6SsQ     0 2014-09-05      0  jsDu6QEJHbwP2Blom1PLCA   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      2  The pizza was okay. Not the best I've had. I p...       0   \n",
       "1      5  I love this place! My fiance And I go here atl...       0   \n",
       "2      1  Terrible. Dry corn bread. Rib tips were all fa...       3   \n",
       "3      2  Back in 2005-2007 this place was my FAVORITE t...       2   \n",
       "4      5  Delicious healthy food. The steak is amazing. ...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  msQe1u7Z_XuqjGoqhB0J5g  \n",
       "1  msQe1u7Z_XuqjGoqhB0J5g  \n",
       "2  msQe1u7Z_XuqjGoqhB0J5g  \n",
       "3  msQe1u7Z_XuqjGoqhB0J5g  \n",
       "4  msQe1u7Z_XuqjGoqhB0J5g  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "df = []\n",
    "for chunk in review_reader:\n",
    "    #print(chunk)\n",
    "    df.append(pd.DataFrame(chunk))\n",
    "    i = i+1\n",
    "    if i==100:\n",
    "        break\n",
    "review_data = pd.concat(df)\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iCQpiavjjPzJ5_3gPD5Ebg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>The pizza was okay. Not the best I've had. I p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pomGBqfbxcqPv14c3XH-ZQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this place! My fiance And I go here atl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jtQARsP6P-LbkyjbO1qNGg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible. Dry corn bread. Rib tips were all fa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elqbBhBfElMNSrjFqW3now</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Back in 2005-2007 this place was my FAVORITE t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ums3gaP2qM3W1XcA5r6SsQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Delicious healthy food. The steak is amazing. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool  funny  stars  \\\n",
       "0  iCQpiavjjPzJ5_3gPD5Ebg     0      0      2   \n",
       "1  pomGBqfbxcqPv14c3XH-ZQ     0      0      5   \n",
       "2  jtQARsP6P-LbkyjbO1qNGg     1      1      1   \n",
       "3  elqbBhBfElMNSrjFqW3now     0      0      2   \n",
       "4  Ums3gaP2qM3W1XcA5r6SsQ     0      0      5   \n",
       "\n",
       "                                                text  useful  \n",
       "0  The pizza was okay. Not the best I've had. I p...       0  \n",
       "1  I love this place! My fiance And I go here atl...       0  \n",
       "2  Terrible. Dry corn bread. Rib tips were all fa...       3  \n",
       "3  Back in 2005-2007 this place was my FAVORITE t...       2  \n",
       "4  Delicious healthy food. The steak is amazing. ...       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = review_data.drop(['review_id', 'user_id', 'date'], axis=1)\n",
    "review_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting one review (testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this place! My fiance And I go here atleast once a week. The portions are huge! Food is amazing. I love their carne asada. They have great lunch specials... Leticia is super nice and cares about what you think of her restaurant. You have to try their cheese enchiladas too the sauce is different And amazing!!!\n"
     ]
    }
   ],
   "source": [
    "review = review_data['text'][1]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this place! My fiance And I go here atleast once a week\n",
      " The portions are huge! Food is amazing\n",
      " I love their carne asada\n",
      " They have great lunch specials\n",
      "\n",
      "\n",
      " Leticia is super nice and cares about what you think of her restaurant\n",
      " You have to try their cheese enchiladas too the sauce is different And amazing!!!\n"
     ]
    }
   ],
   "source": [
    "def showStr(st):\n",
    "    review_strings = st.split('.')\n",
    "    for i in review_strings:\n",
    "        print(i)\n",
    "\n",
    "showStr(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test #1: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "#nltk.download()\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    " \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "#negcutoff = int(len(negfeats)*3/4)\n",
    "#poscutoff = int(len(posfeats)*3/4)\n",
    " \n",
    "#trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "trainfeats = negfeats + posfeats\n",
    "#testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data into desired format to feed in classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "#print(testfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this place! My fiance And I go here atleast once a week', ' The portions are huge! Food is amazing', ' I love their carne asada', ' They have great lunch specials', '', '', ' Leticia is super nice and cares about what you think of her restaurant', ' You have to try their cheese enchiladas too the sauce is different And amazing!!!']\n"
     ]
    }
   ],
   "source": [
    "dat = review_strings\n",
    "print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'place!': True, 'I': True, 'a': True, 'week': True, 'here': True, 'love': True, 'go': True, 'fiance': True, 'And': True, 'this': True, 'once': True, 'atleast': True, 'My': True}, 'pos'), ({'': True, 'is': True, 'Food': True, 'amazing': True, 'The': True, 'huge!': True, 'portions': True, 'are': True}, 'pos'), ({'': True, 'love': True, 'I': True, 'asada': True, 'carne': True, 'their': True}, 'pos'), ({'': True, 'great': True, 'They': True, 'specials': True, 'lunch': True, 'have': True}, 'pos'), ({'': True}, 'pos'), ({'': True}, 'pos'), ({'': True, 'of': True, 'think': True, 'super': True, 'her': True, 'nice': True, 'what': True, 'you': True, 'and': True, 'cares': True, 'restaurant': True, 'about': True, 'is': True, 'Leticia': True}, 'pos'), ({'': True, 'amazing!!!': True, 'the': True, 'You': True, 'enchiladas': True, 'to': True, 'different': True, 'try': True, 'And': True, 'cheese': True, 'is': True, 'have': True, 'sauce': True, 'their': True, 'too': True}, 'pos')]\n"
     ]
    }
   ],
   "source": [
    "dat1 = []\n",
    "for i in dat:\n",
    "    words = i.split(' ')\n",
    "    dic = ({w:True for w in words}, 'pos')\n",
    "    dat1.append(dic)\n",
    "print(dat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 2000 instances, test on 8 instances\n",
      "accuracy: 0.625\n",
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     13.0 : 1.0\n",
      "              astounding = True              pos : neg    =     12.3 : 1.0\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.5 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.0 : 1.0\n",
      "             fascination = True              pos : neg    =     11.0 : 1.0\n",
      "               insulting = True              neg : pos    =     11.0 : 1.0\n",
      "                    3000 = True              neg : pos    =     11.0 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.6 : 1.0\n",
      "                seamless = True              pos : neg    =     10.3 : 1.0\n",
      "\n",
      "I love this place! My fiance And I go here atleast once a week\n",
      " The portions are huge! Food is amazing\n",
      " I love their carne asada\n",
      " They have great lunch specials\n",
      "\n",
      "\n",
      " Leticia is super nice and cares about what you think of her restaurant\n",
      " You have to try their cheese enchiladas too the sauce is different And amazing!!!\n"
     ]
    }
   ],
   "source": [
    "testfeats = dat1\n",
    "print('train on '+str(len(trainfeats))+' instances, test on '+str(len(testfeats))+' instances')\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy: ' + str(nltk.classify.util.accuracy(classifier, testfeats)))\n",
    "classifier.show_most_informative_features()\n",
    "\n",
    "print()\n",
    "showStr(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement: stopwords and collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.625\n",
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     13.0 : 1.0\n",
      "              astounding = True              pos : neg    =     12.3 : 1.0\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.5 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.0 : 1.0\n",
      "             fascination = True              pos : neg    =     11.0 : 1.0\n",
      "               insulting = True              neg : pos    =     11.0 : 1.0\n",
      "                    3000 = True              neg : pos    =     11.0 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.6 : 1.0\n",
      "                seamless = True              pos : neg    =     10.3 : 1.0\n",
      "\n",
      "I love this place! My fiance And I go here atleast once a week\n",
      " The portions are huge! Food is amazing\n",
      " I love their carne asada\n",
      " They have great lunch specials\n",
      "\n",
      "\n",
      " Leticia is super nice and cares about what you think of her restaurant\n",
      " You have to try their cheese enchiladas too the sauce is different And amazing!!!\n"
     ]
    }
   ],
   "source": [
    "#import collections\n",
    "#import nltk.classify.util, nltk.metrics\n",
    "#from nltk.classify import NaiveBayesClassifier\n",
    "#from nltk.corpus import movie_reviews\n",
    "\n",
    "def evaluate_classifier(featx):\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    " \n",
    "    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "    \n",
    "    #print(negfeats)\n",
    " \n",
    "    #negcutoff = len(negfeats)*3/4\n",
    "    #poscutoff = len(posfeats)*3/4\n",
    " \n",
    "    trainfeats = negfeats + posfeats\n",
    "    #testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "    testfeats = dat1\n",
    " \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    " \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    " \n",
    "    print( 'accuracy:' + str( nltk.classify.util.accuracy(classifier, testfeats) ) )\n",
    "    #print( 'pos precision:' + str( nltk.metrics.precision(refsets['pos'], testsets['pos']) ) )\n",
    "    #print( 'pos recall:' + str( nltk.metrics.recall(refsets['pos'], testsets['pos']) ) )\n",
    "    #print( 'neg precision:' + str( nltk.metrics.precision(refsets['neg'], testsets['neg']) ) )\n",
    "    #print( 'neg recall:' + str( nltk.metrics.recall(refsets['neg'], testsets['neg']) ) )\n",
    "    classifier.show_most_informative_features()\n",
    "    \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "evaluate_classifier(word_feats)\n",
    "\n",
    "print()\n",
    "showStr(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High info feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating single word features\n",
      "accuracy:0.625\n",
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     13.0 : 1.0\n",
      "              astounding = True              pos : neg    =     12.3 : 1.0\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.5 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.0 : 1.0\n",
      "             fascination = True              pos : neg    =     11.0 : 1.0\n",
      "               insulting = True              neg : pos    =     11.0 : 1.0\n",
      "                    3000 = True              neg : pos    =     11.0 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.6 : 1.0\n",
      "                seamless = True              pos : neg    =     10.3 : 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('\"', 331.5484150481821) is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-45338aef00d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mbestwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-45338aef00d9>\u001b[0m in \u001b[0;36mfindKey\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfindKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('\"', 331.5484150481821) is not in list"
     ]
    }
   ],
   "source": [
    "import collections, itertools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    " \n",
    "def evaluate_classifier(featx):\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    " \n",
    "    negfeats = [(featx(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "    posfeats = [(featx(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    " \n",
    "    #negcutoff = len(negfeats)*3/4\n",
    "    #poscutoff = len(posfeats)*3/4\n",
    " \n",
    "    trainfeats = negfeats + posfeats\n",
    "    testfeats = dat1 #negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    " \n",
    "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    " \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    " \n",
    "    print( 'accuracy:' + str( nltk.classify.util.accuracy(classifier, testfeats) ) )\n",
    "    classifier.show_most_informative_features()\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "print('evaluating single word features')\n",
    "evaluate_classifier(word_feats)\n",
    " \n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "for word in movie_reviews.words(categories=['pos']):\n",
    "    word_fd.update(word.lower())\n",
    "    label_word_fd['pos'].update(word.lower())\n",
    " \n",
    "for word in movie_reviews.words(categories=['neg']):\n",
    "    word_fd.update(word.lower())\n",
    "    label_word_fd['neg'].update(word.lower())\n",
    " \n",
    "# n_ii = label_word_fd[label][word]\n",
    "# n_ix = word_fd[word]\n",
    "# n_xi = label_word_fd[label].N()\n",
    "# n_xx = label_word_fd.N()\n",
    " \n",
    "pos_word_count = label_word_fd['pos'].N()\n",
    "neg_word_count = label_word_fd['neg'].N()\n",
    "total_word_count = pos_word_count + neg_word_count\n",
    " \n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.items():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "        (freq, pos_word_count), total_word_count)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "        (freq, neg_word_count), total_word_count)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "\n",
    "def findKey(x):\n",
    "    return list(word_scores.keys())[list(word_scores.values()).index(x)]\n",
    "\n",
    "best = sorted(word_scores.items(), key = findKey, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])\n",
    " \n",
    "def best_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    " \n",
    "print('evaluating best word features')\n",
    "evaluate_classifier(best_word_feats)\n",
    " \n",
    "def best_bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    d = dict([(bigram, True) for bigram in bigrams])\n",
    "    d.update(best_word_feats(words))\n",
    "    return d\n",
    " \n",
    "print('evaluating best words + bigram chi_sq word features')\n",
    "evaluate_classifier(best_bigram_word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
