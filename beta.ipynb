{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iCQpiavjjPzJ5_3gPD5Ebg</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-25</td>\n",
       "      <td>0</td>\n",
       "      <td>x7mDIiDB3jEiPGPHOmDzyw</td>\n",
       "      <td>2</td>\n",
       "      <td>The pizza was okay. Not the best I've had. I p...</td>\n",
       "      <td>0</td>\n",
       "      <td>msQe1u7Z_XuqjGoqhB0J5g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  iCQpiavjjPzJ5_3gPD5Ebg     0 2011-02-25      0  x7mDIiDB3jEiPGPHOmDzyw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      2  The pizza was okay. Not the best I've had. I p...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  msQe1u7Z_XuqjGoqhB0J5g  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_reader = pd.read_json('yelp_academic_dataset_review.json', chunksize=1, lines=True)\n",
    "i = 0\n",
    "business_data = pd.DataFrame([])\n",
    "for chunk in review_reader:\n",
    "    #print(chunk)\n",
    "    business_data = chunk\n",
    "    i = i+1\n",
    "    if i==1:\n",
    "        break\n",
    "#business_data = pd.concat(df)\n",
    "#print(type(chunk))\n",
    "business_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iCQpiavjjPzJ5_3gPD5Ebg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_id = business_data['business_id']\n",
    "business_id = business_id[0]\n",
    "business_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"review_reader = pd.read_json('yelp_academic_dataset_review.json', chunksize=1, lines=True)\\ni = 0\\nj = 0\\ndf = []\\nfor chunk in review_reader:\\n    #for c in chunk:\\n    if chunk['business_id'][j] == business_id:\\n        #print(chunk['business_id'][j])\\n        df.append(chunk)\\n        i = i+1\\n        if i==200:\\n            break\\n    j = j+1\\n    #if j==10000:\\n    #    break\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''review_reader = pd.read_json('yelp_academic_dataset_review.json', chunksize=1, lines=True)\n",
    "i = 0\n",
    "j = 0\n",
    "df = []\n",
    "for chunk in review_reader:\n",
    "    #for c in chunk:\n",
    "    if chunk['business_id'][j] == business_id:\n",
    "        #print(chunk['business_id'][j])\n",
    "        df.append(chunk)\n",
    "        i = i+1\n",
    "        if i==200:\n",
    "            break\n",
    "    j = j+1\n",
    "    #if j==10000:\n",
    "    #    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 395 entries, 0 to 394\n",
      "Data columns (total 10 columns):\n",
      "Unnamed: 0     395 non-null int64\n",
      "business_id    395 non-null object\n",
      "cool           395 non-null int64\n",
      "date           395 non-null object\n",
      "funny          395 non-null int64\n",
      "review_id      395 non-null object\n",
      "stars          395 non-null int64\n",
      "text           395 non-null object\n",
      "useful         395 non-null int64\n",
      "user_id        395 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 30.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#review_data = pd.concat(df)\n",
    "review_data = pd.read_csv('review_data.csv')\n",
    "review_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data.to_csv('review_data.csv')\n",
    "#review_text.to_json('review_text.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x7mDIiDB3jEiPGPHOmDzyw</td>\n",
       "      <td>The pizza was okay. Not the best I've had. I p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VluIpojME0yKOcRKI5L0Iw</td>\n",
       "      <td>came here on Monday, no line. its good, but no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nsB9JAeghk0zOaSulSm9Yw</td>\n",
       "      <td>This place is truly a secret!  Its so hidden t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pcn01EMERpCpHEcoaohdEg</td>\n",
       "      <td>This SECRET  PIZZA was a secret for about 5 st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0mFtAPTmInbXHqDjX9eiOg</td>\n",
       "      <td>this is the BEST place to grab a quick slice o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                                               text\n",
       "0  x7mDIiDB3jEiPGPHOmDzyw  The pizza was okay. Not the best I've had. I p...\n",
       "1  VluIpojME0yKOcRKI5L0Iw  came here on Monday, no line. its good, but no...\n",
       "2  nsB9JAeghk0zOaSulSm9Yw  This place is truly a secret!  Its so hidden t...\n",
       "3  pcn01EMERpCpHEcoaohdEg  This SECRET  PIZZA was a secret for about 5 st...\n",
       "4  0mFtAPTmInbXHqDjX9eiOg  this is the BEST place to grab a quick slice o..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = review_data.loc[:,['review_id','text']]\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pizza was okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the best I've had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I prefer Biaggio's on Flamingo / Fort Apache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The chef there can make a MUCH better NY style...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The pizzeria @ Cosmo was over priced for the q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                 The pizza was okay\n",
       "1                              Not the best I've had\n",
       "2       I prefer Biaggio's on Flamingo / Fort Apache\n",
       "3  The chef there can make a MUCH better NY style...\n",
       "4  The pizzeria @ Cosmo was over priced for the q..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import re\n",
    "txt = []\n",
    "for index,row in reviews.iterrows():\n",
    "    text = row['text']\n",
    "    text = text.split('. ')\n",
    "    txt.extend(text)\n",
    "txt\n",
    "review_text = pd.DataFrame(txt, columns=['text'])\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pizza was okay</td>\n",
       "      <td>[the, pizza, was, okay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the best I've had</td>\n",
       "      <td>[not, the, best, i, 've, had]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I prefer Biaggio's on Flamingo / Fort Apache</td>\n",
       "      <td>[i, prefer, biaggio, 's, on, flamingo, /, fort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The chef there can make a MUCH better NY style...</td>\n",
       "      <td>[the, chef, there, can, make, a, much, better,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The pizzeria @ Cosmo was over priced for the q...</td>\n",
       "      <td>[the, pizzeria, @, cosmo, was, over, priced, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                 The pizza was okay   \n",
       "1                              Not the best I've had   \n",
       "2       I prefer Biaggio's on Flamingo / Fort Apache   \n",
       "3  The chef there can make a MUCH better NY style...   \n",
       "4  The pizzeria @ Cosmo was over priced for the q...   \n",
       "\n",
       "                                              tokens  \n",
       "0                            [the, pizza, was, okay]  \n",
       "1                      [not, the, best, i, 've, had]  \n",
       "2  [i, prefer, biaggio, 's, on, flamingo, /, fort...  \n",
       "3  [the, chef, there, can, make, a, much, better,...  \n",
       "4  [the, pizzeria, @, cosmo, was, over, priced, f...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "token_list = []\n",
    "for index,row in review_text.iterrows():\n",
    "    text = row['text'].lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    token_list.append([tokens])\n",
    "    #print(token_list)\n",
    "token_col = pd.DataFrame(token_list, columns=['tokens'])\n",
    "#print(token_list)\n",
    "#token_col.head()\n",
    "review_text = pd.concat([review_text, token_col], axis=1)\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal and conversion to lowercase\n",
    "### (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#nltk.download('stopwords')\\nfrom nltk.corpus import stopwords\\nstop_words = set(stopwords.words('english')) \\nftext = []\\nfor index,row in review_text.iterrows():\\n    tokens = row['tokens']\\n    filtered_text = [w.lower() for w in tokens]#if not w in stop_words] \\n    ftext.append([filtered_text])\\nfiltered_col = pd.DataFrame(ftext, columns=['filtered_tokens'])\\nreview_text = pd.concat([review_text, filtered_col], axis=1)\\nreview_text.head()\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "ftext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    tokens = row['tokens']\n",
    "    filtered_text = [w.lower() for w in tokens]#if not w in stop_words] \n",
    "    ftext.append([filtered_text])\n",
    "filtered_col = pd.DataFrame(ftext, columns=['filtered_tokens'])\n",
    "review_text = pd.concat([review_text, filtered_col], axis=1)\n",
    "review_text.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\nttext = []\\nfor index,row in review_text.iterrows():\\n    filtered_text = row['tokens']\\n    lemmatized_tokens = []\\n    for ftr in filtered_text:\\n        lf = lemmatizer.lemmatize(ftr)\\n        lemmatized_tokens.append(lf)\\n    ttext.append([lemmatized_tokens])\\ntagged_col = pd.DataFrame(ttext, columns=['lemmatized_tokens'])\\nreview_text = pd.concat([review_text, tagged_col], axis=1)\\nreview_text.head()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ttext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    filtered_text = row['tokens']\n",
    "    lemmatized_tokens = []\n",
    "    for ftr in filtered_text:\n",
    "        lf = lemmatizer.lemmatize(ftr)\n",
    "        lemmatized_tokens.append(lf)\n",
    "    ttext.append([lemmatized_tokens])\n",
    "tagged_col = pd.DataFrame(ttext, columns=['lemmatized_tokens'])\n",
    "review_text = pd.concat([review_text, tagged_col], axis=1)\n",
    "review_text.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging using upenn tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pizza was okay</td>\n",
       "      <td>[the, pizza, was, okay]</td>\n",
       "      <td>[(the, DT), (pizza, NN), (was, VBD), (okay, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the best I've had</td>\n",
       "      <td>[not, the, best, i, 've, had]</td>\n",
       "      <td>[(not, RB), (the, DT), (best, JJS), (i, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I prefer Biaggio's on Flamingo / Fort Apache</td>\n",
       "      <td>[i, prefer, biaggio, 's, on, flamingo, /, fort...</td>\n",
       "      <td>[(i, NN), (prefer, VBP), (biaggio, NN), ('s, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The chef there can make a MUCH better NY style...</td>\n",
       "      <td>[the, chef, there, can, make, a, much, better,...</td>\n",
       "      <td>[(the, DT), (chef, NN), (there, EX), (can, MD)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The pizzeria @ Cosmo was over priced for the q...</td>\n",
       "      <td>[the, pizzeria, @, cosmo, was, over, priced, f...</td>\n",
       "      <td>[(the, DT), (pizzeria, NN), (@, NNP), (cosmo, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                 The pizza was okay   \n",
       "1                              Not the best I've had   \n",
       "2       I prefer Biaggio's on Flamingo / Fort Apache   \n",
       "3  The chef there can make a MUCH better NY style...   \n",
       "4  The pizzeria @ Cosmo was over priced for the q...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                            [the, pizza, was, okay]   \n",
       "1                      [not, the, best, i, 've, had]   \n",
       "2  [i, prefer, biaggio, 's, on, flamingo, /, fort...   \n",
       "3  [the, chef, there, can, make, a, much, better,...   \n",
       "4  [the, pizzeria, @, cosmo, was, over, priced, f...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0   [(the, DT), (pizza, NN), (was, VBD), (okay, JJ)]  \n",
       "1  [(not, RB), (the, DT), (best, JJS), (i, NN), (...  \n",
       "2  [(i, NN), (prefer, VBP), (biaggio, NN), ('s, P...  \n",
       "3  [(the, DT), (chef, NN), (there, EX), (can, MD)...  \n",
       "4  [(the, DT), (pizzeria, NN), (@, NNP), (cosmo, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "ttext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    filtered_text = row['tokens']\n",
    "    tagged_text = nltk.pos_tag(filtered_text) \n",
    "    ttext.append([tagged_text])\n",
    "tagged_col = pd.DataFrame(ttext, columns=['tagged_tokens'])\n",
    "review_text = pd.concat([review_text, tagged_col], axis=1)\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('tagsets')\n",
    "#nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun / noun phrase filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The pizza was okay</td>\n",
       "      <td>[the, pizza, was, okay]</td>\n",
       "      <td>[(the, DT), (pizza, NN), (was, VBD), (okay, JJ)]</td>\n",
       "      <td>[pizza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the best I've had</td>\n",
       "      <td>[not, the, best, i, 've, had]</td>\n",
       "      <td>[(not, RB), (the, DT), (best, JJS), (i, NN), (...</td>\n",
       "      <td>[i]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I prefer Biaggio's on Flamingo / Fort Apache</td>\n",
       "      <td>[i, prefer, biaggio, 's, on, flamingo, /, fort...</td>\n",
       "      <td>[(i, NN), (prefer, VBP), (biaggio, NN), ('s, P...</td>\n",
       "      <td>[i, biaggio, flamingo, /, fort, apache]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The chef there can make a MUCH better NY style...</td>\n",
       "      <td>[the, chef, there, can, make, a, much, better,...</td>\n",
       "      <td>[(the, DT), (chef, NN), (there, EX), (can, MD)...</td>\n",
       "      <td>[chef, style, pizza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The pizzeria @ Cosmo was over priced for the q...</td>\n",
       "      <td>[the, pizzeria, @, cosmo, was, over, priced, f...</td>\n",
       "      <td>[(the, DT), (pizzeria, NN), (@, NNP), (cosmo, ...</td>\n",
       "      <td>[pizzeria, @, cosmo, quality, lack, personalit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                 The pizza was okay   \n",
       "1                              Not the best I've had   \n",
       "2       I prefer Biaggio's on Flamingo / Fort Apache   \n",
       "3  The chef there can make a MUCH better NY style...   \n",
       "4  The pizzeria @ Cosmo was over priced for the q...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                            [the, pizza, was, okay]   \n",
       "1                      [not, the, best, i, 've, had]   \n",
       "2  [i, prefer, biaggio, 's, on, flamingo, /, fort...   \n",
       "3  [the, chef, there, can, make, a, much, better,...   \n",
       "4  [the, pizzeria, @, cosmo, was, over, priced, f...   \n",
       "\n",
       "                                       tagged_tokens  \\\n",
       "0   [(the, DT), (pizza, NN), (was, VBD), (okay, JJ)]   \n",
       "1  [(not, RB), (the, DT), (best, JJS), (i, NN), (...   \n",
       "2  [(i, NN), (prefer, VBP), (biaggio, NN), ('s, P...   \n",
       "3  [(the, DT), (chef, NN), (there, EX), (can, MD)...   \n",
       "4  [(the, DT), (pizzeria, NN), (@, NNP), (cosmo, ...   \n",
       "\n",
       "                                               nouns  \n",
       "0                                            [pizza]  \n",
       "1                                                [i]  \n",
       "2            [i, biaggio, flamingo, /, fort, apache]  \n",
       "3                               [chef, style, pizza]  \n",
       "4  [pizzeria, @, cosmo, quality, lack, personalit...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    tagged_text = row['tagged_tokens']\n",
    "    nouns = []\n",
    "    for tt in tagged_text:\n",
    "        tag = tt[1]\n",
    "        if tag in ['NN','NNS','NNP','NNPS']:\n",
    "            nouns.append(tt[0])\n",
    "    ntext.append([nouns])\n",
    "noun_col = pd.DataFrame(ntext, columns=['nouns'])\n",
    "noun_col\n",
    "review_text = pd.concat([review_text, noun_col], axis=1)\n",
    "review_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "### (not including in procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.stem import PorterStemmer\\nps = PorterStemmer()\\nfor w in nouns:\\n    print(w + '\\t:\\t' + ps.stem(w))\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for w in nouns:\n",
    "    print(w + '\\t:\\t' + ps.stem(w))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching (example)\n",
    "### (not including in procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from fuzzywuzzy import fuzz\\nfrom fuzzywuzzy import process\\nquery = 'Barack Obama'\\nchoices = ['Barack H Obama', 'Barack H. Obama', 'B. Obama']\\n# Get a list of matches ordered by score, default limit to 5\\nprocess.extract(query, choices)\\n# [('Barack H Obama', 95), ('Barack H. Obama', 95), ('B. Obama', 85)]\\n \\n# If we want only the top one\\nprocess.extractOne(query, choices)\\n# ('Barack H Obama', 95)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "query = 'Barack Obama'\n",
    "choices = ['Barack H Obama', 'Barack H. Obama', 'B. Obama']\n",
    "# Get a list of matches ordered by score, default limit to 5\n",
    "process.extract(query, choices)\n",
    "# [('Barack H Obama', 95), ('Barack H. Obama', 95), ('B. Obama', 85)]\n",
    " \n",
    "# If we want only the top one\n",
    "process.extractOne(query, choices)\n",
    "# ('Barack H Obama', 95)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing processed data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text.to_csv('review_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running additional noun filtering\n",
    "### (not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ttext = []\\nfor index,row in review_text.iterrows():\\n    filtered_text = row['nouns']\\n    tagged_text = nltk.pos_tag(filtered_text) \\n    ttext.append([tagged_text])\\ntagged_col = pd.DataFrame(ttext, columns=['tagged_noun_tokens'])\\nreview_text = pd.concat([review_text, tagged_col], axis=1)\\nreview_text.head()\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ttext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    filtered_text = row['nouns']\n",
    "    tagged_text = nltk.pos_tag(filtered_text) \n",
    "    ttext.append([tagged_text])\n",
    "tagged_col = pd.DataFrame(ttext, columns=['tagged_noun_tokens'])\n",
    "review_text = pd.concat([review_text, tagged_col], axis=1)\n",
    "review_text.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ntext = []\\nfor index,row in review_text.iterrows():\\n    tagged_text = row['tagged_noun_tokens']\\n    nouns = []\\n    for tt in tagged_text:\\n        tag = tt[1]\\n        if tag in ['NN','NNS','NNP','NNPS']:\\n            nouns.append(tt[0])\\n    ntext.append([nouns])\\nnoun_col = pd.DataFrame(ntext, columns=['filtered_nouns'])\\nnoun_col\\nreview_text = pd.concat([review_text, noun_col], axis=1)\\nreview_text.head()\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ntext = []\n",
    "for index,row in review_text.iterrows():\n",
    "    tagged_text = row['tagged_noun_tokens']\n",
    "    nouns = []\n",
    "    for tt in tagged_text:\n",
    "        tag = tt[1]\n",
    "        if tag in ['NN','NNS','NNP','NNPS']:\n",
    "            nouns.append(tt[0])\n",
    "    ntext.append([nouns])\n",
    "noun_col = pd.DataFrame(ntext, columns=['filtered_nouns'])\n",
    "noun_col\n",
    "review_text = pd.concat([review_text, noun_col], axis=1)\n",
    "review_text.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Frequent features identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns = review_text['nouns']\n",
    "filtered_nouns = filtered_nouns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pizza'],\n",
       " ['i'],\n",
       " ['i', 'biaggio', 'flamingo', '/', 'fort', 'apache'],\n",
       " ['chef', 'style', 'pizza'],\n",
       " ['pizzeria', '@', 'cosmo', 'quality', 'lack', 'personality', 'food']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_nouns = []\n",
    "for fn in filtered_nouns:\n",
    "    lf = []\n",
    "    for f in fn:\n",
    "        lf.append(lemmatizer.lemmatize(f))\n",
    "    lemm_nouns.append(lf)\n",
    "lemm_nouns[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "#print(features.keys())\n",
    "for nouns in lemm_nouns:\n",
    "    keys = features.keys()\n",
    "    visited = []\n",
    "    for n in nouns:\n",
    "        if n not in visited:\n",
    "            if n in keys:\n",
    "                features[n] += 1\n",
    "            else:\n",
    "                features.update({n: 1})\n",
    "\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spot',\n",
       " 'strip',\n",
       " 'floor',\n",
       " 'vega',\n",
       " 'minute',\n",
       " 'topping',\n",
       " 'price',\n",
       " 'crust',\n",
       " 'sausage',\n",
       " 'thing',\n",
       " 'ricotta',\n",
       " 'i',\n",
       " 'night',\n",
       " 'slice',\n",
       " 'staff',\n",
       " 'area',\n",
       " 'pizza',\n",
       " 'restaurant',\n",
       " 'cosmo',\n",
       " 'place',\n",
       " 'order',\n",
       " 'wait',\n",
       " 'york',\n",
       " 'service',\n",
       " 'cosmopolitan',\n",
       " 'hallway',\n",
       " 'line',\n",
       " 'sign',\n",
       " 'style',\n",
       " 'way',\n",
       " 'friend',\n",
       " 'pepperoni',\n",
       " 'pie',\n",
       " 'food',\n",
       " 'time',\n",
       " 'cheese',\n",
       " 'people']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ftr = []\n",
    "print(review_text['text'].count())\n",
    "threshold = int(review_text['text'].count() * 0.015)\n",
    "for k,v in features.items():\n",
    "    if v >= threshold:\n",
    "        frequent_ftr.append(k)\n",
    "frequent_ftr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Opinion words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"opinion_words = {}\\nfor index,row in review_text.iterrows():\\n    op_words = []\\n    for ftr in frequent_ftr:\\n        if ftr in row['tokens']:\\n            for tt in row['tagged_tokens']:\\n                tag = tt[1]\\n                word = tt[0]\\n                if (tag in ['JJ','JJR','JJS']) and (word not in op_words):\\n                    op_words.append(tt[0])\\n    opinion_words.update({ftr:op_words})\\n\\nopinion_words\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''opinion_words = {}\n",
    "for index,row in review_text.iterrows():\n",
    "    op_words = []\n",
    "    for ftr in frequent_ftr:\n",
    "        if ftr in row['tokens']:\n",
    "            for tt in row['tagged_tokens']:\n",
    "                tag = tt[1]\n",
    "                word = tt[0]\n",
    "                if (tag in ['JJ','JJR','JJS']) and (word not in op_words):\n",
    "                    op_words.append(tt[0])\n",
    "    opinion_words.update({ftr:op_words})\n",
    "\n",
    "opinion_words'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "#print(documents[1])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "#print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 71.77289769683985\n",
      "Most Informative Features\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "                  feeble = True              neg : pos    =      8.3 : 1.0\n",
      "                 symbols = True              pos : neg    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.4 : 1.0\n",
      "                supports = True              pos : neg    =      7.0 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.6 : 1.0\n",
      "                  skimpy = True              neg : pos    =      6.3 : 1.0\n",
      "                  elmore = True              pos : neg    =      6.3 : 1.0\n",
      "                   masks = True              neg : pos    =      5.7 : 1.0\n",
      "                  purple = True              pos : neg    =      5.7 : 1.0\n",
      "                 kidnaps = True              neg : pos    =      5.7 : 1.0\n",
      "              dedication = True              pos : neg    =      5.7 : 1.0\n",
      "                 warrant = True              neg : pos    =      5.4 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.1 : 1.0\n",
      "                   fluke = True              neg : pos    =      5.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# set that we'll train our classifier with\n",
    "training_set = featuresets\n",
    "\n",
    "# set that we'll test against.\n",
    "#testing_set = featuresets[1900:]\n",
    "dat1 = []\n",
    "for index,row in review_text.iterrows():\n",
    "    words = row['tokens']\n",
    "    dic = ({w:True for w in words}, 'pos')\n",
    "    dat1.append(dic)\n",
    "\n",
    "testing_set = dat1\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 71.77289769683985\n",
      "Most Informative Features\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "                  feeble = True              neg : pos    =      8.3 : 1.0\n",
      "                 symbols = True              pos : neg    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.4 : 1.0\n",
      "                supports = True              pos : neg    =      7.0 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.6 : 1.0\n",
      "                  skimpy = True              neg : pos    =      6.3 : 1.0\n",
      "                  elmore = True              pos : neg    =      6.3 : 1.0\n",
      "                   masks = True              neg : pos    =      5.7 : 1.0\n",
      "                  purple = True              pos : neg    =      5.7 : 1.0\n",
      "                 kidnaps = True              neg : pos    =      5.7 : 1.0\n",
      "              dedication = True              pos : neg    =      5.7 : 1.0\n",
      "                 warrant = True              neg : pos    =      5.4 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.1 : 1.0\n",
      "                   fluke = True              neg : pos    =      5.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.20085698982324585\n",
      "BernoulliNB accuracy percent: 0.05088377075522228\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNB_classifier, testing_set))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNB_classifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 71.77289769683985\n",
      "Most Informative Features\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "                  feeble = True              neg : pos    =      8.3 : 1.0\n",
      "                 symbols = True              pos : neg    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.4 : 1.0\n",
      "                supports = True              pos : neg    =      7.0 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.6 : 1.0\n",
      "                  skimpy = True              neg : pos    =      6.3 : 1.0\n",
      "                  elmore = True              pos : neg    =      6.3 : 1.0\n",
      "                   masks = True              neg : pos    =      5.7 : 1.0\n",
      "                  purple = True              pos : neg    =      5.7 : 1.0\n",
      "                 kidnaps = True              neg : pos    =      5.7 : 1.0\n",
      "              dedication = True              pos : neg    =      5.7 : 1.0\n",
      "                 warrant = True              neg : pos    =      5.4 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.1 : 1.0\n",
      "                   fluke = True              neg : pos    =      5.0 : 1.0\n",
      "MNB_classifier accuracy percent: 20.085698982324583\n",
      "BernoulliNB_classifier accuracy percent: 5.088377075522228\n",
      "LogisticRegression_classifier accuracy percent: 70.96946973754686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 69.8446705945367\n",
      "SVC_classifier accuracy percent: 94.32244242099625\n",
      "LinearSVC_classifier accuracy percent: 80.02142474558114\n",
      "NuSVC_classifier accuracy percent: 20.032137118371722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 71.77289769683985\n",
      "Most Informative Features\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "                  feeble = True              neg : pos    =      8.3 : 1.0\n",
      "                 symbols = True              pos : neg    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.4 : 1.0\n",
      "                supports = True              pos : neg    =      7.0 : 1.0\n",
      "                    taxi = True              pos : neg    =      6.6 : 1.0\n",
      "                  skimpy = True              neg : pos    =      6.3 : 1.0\n",
      "                  elmore = True              pos : neg    =      6.3 : 1.0\n",
      "                   masks = True              neg : pos    =      5.7 : 1.0\n",
      "                  purple = True              pos : neg    =      5.7 : 1.0\n",
      "                 kidnaps = True              neg : pos    =      5.7 : 1.0\n",
      "              dedication = True              pos : neg    =      5.7 : 1.0\n",
      "                 warrant = True              neg : pos    =      5.4 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.1 : 1.0\n",
      "                   fluke = True              neg : pos    =      5.0 : 1.0\n",
      "MNB_classifier accuracy percent: 20.085698982324583\n",
      "BernoulliNB_classifier accuracy percent: 5.088377075522228\n",
      "LogisticRegression_classifier accuracy percent: 70.96946973754686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 28.602035350830207\n",
      "LinearSVC_classifier accuracy percent: 80.02142474558114\n",
      "NuSVC_classifier accuracy percent: 20.032137118371722\n",
      "voted_classifier accuracy percent: 22.603106588109267\n",
      "Classification: neg Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 71.42857142857143\n",
      "Classification: neg Confidence %: 57.14285714285714\n",
      "Classification: neg Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\",\"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk import word_tokenize\n",
    "short_pos = open(\"positive.txt\",\"r\").read()\n",
    "short_neg = open(\"negative.txt\",\"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
